from snakemake.utils import min_version
import os
import csv

# Ensure minimum Snakemake version for compatibility
min_version("6.0")

# Create essential directories if they don't exist
for dir_path in [
    "logs/fastqc", "logs/multiqc", "logs/trimming", 
    "Analysis/QC/FastQC", "Analysis/QC/MultiQC", "Analysis/QC/Trimming",
    "Analysis/QC/Trimming/Reports", "Analysis/QC/Trimming/MultiQC", 
    "Analysis/Trimmed"
]:
    os.makedirs(dir_path, exist_ok=True)

# Load configuration file (using relative path for portability)
configfile: "resources/config/params.yaml"

# Function to parse metasheet and extract sample information
# This centralizes sample handling for all workflow components
def get_samples_from_metasheet():
    samples = {}
    try:
        with open(config["metasheet_path"], "r") as f:
            reader = csv.DictReader(f)
            for row in reader:
                sample_name = row["sample"]
                samples[sample_name] = {
                    "R1": row["R1"],
                    "R2": row["R2"],
                    "srr": os.path.basename(row["R1"]).replace("_R1.fastq.gz", "")  # Get SRR ID from filename
                }
                print(f"Loaded sample: {sample_name}, SRR: {samples[sample_name]['srr']}")
    except FileNotFoundError:
        print(f"Error: Metasheet not found at {config['metasheet_path']}")
        raise
    except KeyError as e:
        print(f"Error: Missing required column in metasheet: {e}")
        raise
    return samples

# Get samples information
SAMPLES = get_samples_from_metasheet()

# Include rules from other Snakefiles
# This brings in all the rules defined in the other files
include: "workflow/qc_params.snakefile"     # QC and parameter generation rules
include: "workflow/fastp_trimming.snakefile"  # Trimming rules

# Define the workflow order
ruleorder: fastqc > multiqc > generate_trimming_params > fastp_with_dependency > multiqc_fastp

# Define the complete workflow with all expected outputs
rule all:
    input:
        # QC stage outputs
        expand("Analysis/QC/FastQC/{sample}/{sample}_R1_fastqc.html",
               sample=SAMPLES.keys()),
        expand("Analysis/QC/FastQC/{sample}/{sample}_R2_fastqc.html",
               sample=SAMPLES.keys()),
        "Analysis/QC/MultiQC/multiqc_report.html",
        "Analysis/QC/Trimming/trimming_params.json",
        # QC completion marker
        "Analysis/QC/.qc_complete",
        # Trimming stage outputs
        expand("Analysis/Trimmed/{sample}/{sample}_R1_trimmed.fastq.gz",
               sample=SAMPLES.keys()),
        expand("Analysis/Trimmed/{sample}/{sample}_R2_trimmed.fastq.gz",
               sample=SAMPLES.keys()),
        "Analysis/QC/Trimming/MultiQC/multiqc_report.html"

# This rule ensures QC is completed before trimming starts
# It acts as a checkpoint between workflow stages
rule qc_complete:
    input:
        fastqc=expand("Analysis/QC/FastQC/{sample}/{sample}_{read}_fastqc.html",
               sample=SAMPLES.keys(), 
               read=["R1", "R2"]),
        multiqc="Analysis/QC/MultiQC/multiqc_report.html",
        params="Analysis/QC/Trimming/trimming_params.json"
    output:
        touch("Analysis/QC/.qc_complete")
    log:
        "logs/workflow/qc_complete.log"
    params:
        time=config.get("qc_complete_time", "00:10:00")
    resources:
        mem_mb=config.get("qc_complete_memory", 1000)
    shell:
        """
        # Create logs directory if it doesn't exist
        mkdir -p $(dirname {log})
        
        # Log QC completion
        echo "QC and parameter generation completed successfully at $(date)" | tee {log}
        echo "FastQC reports generated: $(find Analysis/QC/FastQC -name '*_fastqc.html' | wc -l)" | tee -a {log}
        echo "MultiQC report: {input.multiqc}" | tee -a {log}
        echo "Trimming parameters: {input.params}" | tee -a {log}
        
        # Verify that all required files exist
        if [ ! -f "{input.multiqc}" ]; then
            echo "ERROR: MultiQC report not found" | tee -a {log}
            exit 1
        fi
        
        if [ ! -f "{input.params}" ]; then
            echo "ERROR: Trimming parameters file not found" | tee -a {log}
            exit 1
        fi
        
        # Create a marker file to indicate QC completion
        echo "Creating QC completion marker file" | tee -a {log}
        """

# Modify the original fastp rule to require QC completion
# This ensures proper stage dependencies
ruleorder: fastp_with_dependency > fastp

def get_fastp_params(wildcards, params_file="Analysis/QC/Trimming/trimming_params.json"):
    """
    Get fastp parameters for a sample directly in Python
    
    This function reads the trimming_params.json file generated by the generate_trimming_params rule
    in workflow/qc_params.snakefile. The JSON file contains sample-specific parameters determined
    by analyzing FastQC results.
    
    If the JSON file doesn't exist or doesn't contain parameters for the sample,
    default parameters are used.
    """
    import json
    import re
    import os
    
    # Default parameters
    defaults = {
        "leading_quality": 3,
        "trailing_quality": 3,
        "min_length": 36,
        "sliding_window": "4:15"
    }
    
    # Load parameters file
    sample_params = {}
    try:
        if os.path.exists(params_file):
            with open(params_file) as f:
                all_params = json.load(f)
                
            # First try direct sample name match
            if wildcards.sample in all_params:
                sample_params = all_params[wildcards.sample]
                print(f"Found parameters for sample: {wildcards.sample}")
            else:
                # Try to extract SRR ID from the sample name and use that
                srr_match = re.search(r'(SRR\d+)', wildcards.sample)
                if srr_match and srr_match.group(1) in all_params:
                    srr_id = srr_match.group(1)
                    sample_params = all_params[srr_id]
                    print(f"Found parameters via SRR ID: {srr_id}")
                else:
                    print(f"No parameters found for {wildcards.sample}. Using defaults.")
                    sample_params = defaults
        else:
            print(f"Warning: Parameters file {params_file} not found, using defaults")
            sample_params = defaults
    except Exception as e:
        print(f"Error loading parameters: {str(e)}, using defaults")
        sample_params = defaults
    
    # Build fastp command options
    cmd_parts = []
    
    # Quality filtering parameters
    cmd_parts.append(f"--qualified_quality_phred {sample_params.get('trailing_quality', 3)}")
    cmd_parts.append("--unqualified_percent_limit 40")
    
    # Sliding window parameters
    sliding_window = sample_params.get('sliding_window', '4:15')
    window_size, window_quality = sliding_window.split(':')
    cmd_parts.append("--cut_front")
    cmd_parts.append(f"--cut_front_window_size {window_size}")
    cmd_parts.append(f"--cut_front_mean_quality {window_quality}")
    cmd_parts.append("--cut_tail")
    cmd_parts.append(f"--cut_tail_window_size {window_size}")
    cmd_parts.append(f"--cut_tail_mean_quality {window_quality}")
    
    # Length filtering
    cmd_parts.append(f"--length_required {sample_params.get('min_length', 36)}")
    
    # Add optional flags
    if sample_params.get('deduplication', False):
        cmd_parts.append("--dedup")
    
    if sample_params.get('trim_poly_g', False):
        cmd_parts.append("--trim_poly_g")
    
    if sample_params.get('low_complexity_filter', False):
        cmd_parts.append("--low_complexity_filter")
    
    # Return the complete command string
    return " ".join(cmd_parts)

rule fastp_with_dependency:
    input:
        r1=lambda wildcards: SAMPLES[wildcards.sample]["R1"],
        r2=lambda wildcards: SAMPLES[wildcards.sample]["R2"],
        qc_complete="Analysis/QC/.qc_complete"
    output:
        r1="Analysis/Trimmed/{sample}/{sample}_R1_trimmed.fastq.gz",
        r2="Analysis/Trimmed/{sample}/{sample}_R2_trimmed.fastq.gz",
        html="Analysis/QC/Trimming/Reports/{sample}_fastp.html",
        json="Analysis/QC/Trimming/Reports/{sample}_fastp.json"
    log:
        "logs/trimming/{sample}.log"
    params:
        fastp_opts=get_fastp_params,
        time=config.get("trimming_time", "02:00:00")
    threads: config.get("trimming_threads", 4)
    resources:
        mem_mb=config.get("trimming_memory", 8000)
    shell:
        """
        # Create output directories
        mkdir -p $(dirname {output.r1})
        mkdir -p $(dirname {output.html})
        
        # Debug information
        echo "Processing sample: {wildcards.sample}" >> {log}
        echo "Input R1: {input.r1}" >> {log}
        echo "Input R2: {input.r2}" >> {log}
        echo "Output R1: {output.r1}" >> {log}
        echo "Output R2: {output.r2}" >> {log}
        echo "Parameters: {params.fastp_opts}" >> {log}
        
        # Run fastp with auto-adapter detection and determined parameters
        echo "Running fastp..." >> {log}
        fastp \
            --in1 {input.r1} \
            --in2 {input.r2} \
            --out1 {output.r1} \
            --out2 {output.r2} \
            --html {output.html} \
            --json {output.json} \
            --thread {threads} \
            --detect_adapter_for_pe \
            {params.fastp_opts} \
            >> {log} 2>&1
        
        # Verify outputs
        if [ ! -f "{output.r1}" ] || [ ! -f "{output.r2}" ]; then
            echo "ERROR: Output files were not created properly." >> {log}
            echo "Command failed, check log for details." >> {log}
            exit 1
        fi
        
        echo "Trimming completed successfully" >> {log}
        
        # Print basic stats
        echo "Output file sizes:" >> {log}
        ls -lh {output.r1} {output.r2} >> {log}
        """

# Rules for running specific workflow stages independently
# These allow users to run just a portion of the workflow

# Run just the parameters generation step
rule params_generation:
    input:
        "Analysis/QC/Trimming/trimming_params.json"
    params:
        time=config.get("params_generation_time", "00:05:00")
    resources:
        mem_mb=config.get("params_generation_memory", 1000)
    shell:
        """
        echo "Parameter generation completed successfully"
        """

# Run just the QC stage
rule qc_stage:
    input:
        "Analysis/QC/.qc_complete"
    params:
        time=config.get("qc_stage_time", "00:05:00")
    resources:
        mem_mb=config.get("qc_stage_memory", 1000)
    shell:
        """
        echo "QC stage completed successfully"
        """

# Run just the trimming stage
rule trimming_stage:
    input:
        expand("Analysis/Trimmed/{sample}/{sample}_R1_trimmed.fastq.gz",
               sample=SAMPLES.keys()),
        expand("Analysis/Trimmed/{sample}/{sample}_R2_trimmed.fastq.gz",
               sample=SAMPLES.keys()),
        "Analysis/QC/Trimming/MultiQC/multiqc_report.html"
    params:
        time=config.get("trimming_stage_time", "00:05:00")
    resources:
        mem_mb=config.get("trimming_stage_memory", 1000)
    shell:
        """
        echo "Trimming stage completed successfully"
        """

# Define workflow stages for better control over execution order
rule workflow_stages:
    input:
        qc="Analysis/QC/.qc_complete",
        trimming=expand("Analysis/Trimmed/{sample}/{sample}_R1_trimmed.fastq.gz",
                      sample=SAMPLES.keys())
    output:
        touch("Analysis/.workflow_complete")
    params:
        time=config.get("workflow_stages_time", "00:10:00")
    resources:
        mem_mb=config.get("workflow_stages_memory", 1000)
    shell:
        """
        echo "Workflow completed successfully at $(date)"
        echo "QC stage: Completed"
        echo "Trimming stage: Completed"
        """

# Provide helpful messages on workflow completion or failure
onsuccess:
    print("\nWorkflow completed successfully!")
    print("================================")
    print("Quality control reports: Analysis/QC/MultiQC/multiqc_report.html")
    print("Trimming parameters: Analysis/QC/Trimming/trimming_params.json")
    print("Trimmed reads: Analysis/Trimmed/{sample}/{sample}_R1_trimmed.fastq.gz")
    print("Trimming reports: Analysis/QC/Trimming/Reports/{sample}_fastp.html")
    print("Trimming MultiQC: Analysis/QC/Trimming/MultiQC/multiqc_report.html")
    print("\nTo visualize the workflow graph:")
    print("  snakemake --dag | dot -Tsvg > workflow.svg")

onerror:
    print("\nWorkflow failed!")
    print("===============")
    print("Check log files in the logs/ directory for error details.")
    print("Run with --notemp flag to keep temporary files for debugging.")
    print("Use -n (dry run) and -p (print shell commands) for troubleshooting.")